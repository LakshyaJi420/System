<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Live Face Swap Filter v1.2</title>
  <meta name="viewport" content="width=640, initial-scale=1, user-scalable=no">
  <style>
    body, html { margin:0; padding:0; background:#181a1b; font-family:sans-serif; color:#fff; }
    #topbar {
      position: absolute; top:0; left:0; width:100%; background:rgba(30,30,30,0.96); z-index:20;
      display:flex; align-items:center; justify-content:space-between; padding:12px 20px;
      box-sizing:border-box; min-width: 640px;
    }
    #uploadBtn {
      padding:8px 18px; background:#00b894; border:none; color:#fff; font-weight:bold;
      border-radius:4px; cursor:pointer; font-size:15px; margin-right:10px;
      transition: background .2s;
    }
    #uploadBtn:hover { background:#019875; }
    #status {
      font-size:14px; color:#00b894; margin-left:15px;
    }
    #privacy {
      font-size:14px; color:#b2bec3; margin-left:15px;
    }
    #container {
      position:relative; width:640px; height:480px; margin:80px auto 0 auto; box-shadow:0 4px 18px #000a;
      border-radius:10px; overflow:hidden; background:#23272b;
    }
    video, canvas {
      position:absolute; top:0; left:0; width:640px; height:480px; display:block; border-radius:10px;
    }
    #footer {
      text-align:center; color:#636e72; font-size:13px; margin-top:36px;
    }
    .loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: white;
      background: rgba(0,0,0,0.7);
      padding: 15px 25px;
      border-radius: 5px;
      z-index: 100;
    }
    @media (max-width:700px) {
      #container, video, canvas { width:100vw; height:75vw; max-width:100vw; max-height:75vw; min-width:0; }
      #topbar { min-width: unset; }
    }
  </style>
</head>
<body>
  <div id="topbar">
    <div>
      <button id="uploadBtn">Upload Source Face</button>
      <span id="status">Ready to start...</span>
      <span id="privacy">
        <b>Privacy:</b> All processing is 100% local. No image or video leaves your device.
      </span>
    </div>
    <span style="font-weight:bold; letter-spacing:1px; font-size:15px;">Live Face Swap Filter</span>
  </div>
  <div id="container">
    <div id="loading" class="loading" style="display:none;">Loading models...</div>
    <video id="video" width="640" height="480" autoplay muted playsinline></video>
    <canvas id="overlay" width="640" height="480"></canvas>
  </div>
  <div id="footer">
    &copy; 2025 Live Face Swap Filter &mdash; Created by @thepawkit
  </div>

  <!-- Face-API -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const statusEl = document.getElementById('status');
    const loadingEl = document.getElementById('loading');
    let srcImg = null, srcPts = null;
    let isModelLoaded = false;

    function updateStatus(message, isError = false) {
      statusEl.style.color = isError ? '#ff6b6b' : '#00b894';
      statusEl.textContent = message;
      console.log(isError ? 'Error:' : 'Status:', message);
    }

    // 1) On load, start camera immediately
    window.onload = async () => {
      loadingEl.style.display = 'block';
      updateStatus('Requesting camera access...');

      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: {
            facingMode: "user",
            width: { ideal: 640 },
            height: { ideal: 480 }
          }
        });
        video.srcObject = stream;
        updateStatus('Camera started successfully');
      } catch (err) {
        updateStatus('Camera access error: ' + err.message, true);
        alert('Unable to access camera: ' + err.message);
        return;
      }

      // 2) Load models
      try {
        updateStatus('Loading face detection models...');
        const modelUrl = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights';
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(modelUrl),
          faceapi.nets.faceLandmark68Net.loadFromUri(modelUrl)
        ]);
        isModelLoaded = true;
        loadingEl.style.display = 'none';
        updateStatus('Models loaded! Upload a source face to begin.');
        video.addEventListener('play', swapLoop);
      } catch (err) {
        updateStatus('Error loading models: ' + err.message, true);
        alert('Error loading face detection models. Please check your internet connection and reload.');
      }
    };

    // 3) User uploads source face
    document.getElementById('uploadBtn').addEventListener('click', () => {
      if (!isModelLoaded) {
        updateStatus('Please wait for models to load...', true);
        return;
      }

      const input = document.createElement('input');
      input.type = "file";
      input.accept = "image/*";
      input.onchange = async e => {
        const file = e.target.files[0];
        if (!file) return;

        updateStatus('Processing uploaded image...');

        try {
          const dataURL = await new Promise(res => {
            const r = new FileReader();
            r.onload = () => res(r.result);
            r.readAsDataURL(file);
          });

          srcImg = new Image();
          srcImg.onload = async () => {
            try {
              updateStatus('Detecting face in uploaded image...');
              const det = await faceapi
                .detectSingleFace(srcImg, new faceapi.TinyFaceDetectorOptions())
                .withFaceLandmarks();

              if (!det) {
                updateStatus('No face found in uploaded image. Please try another photo.', true);
                srcImg = null;
                return;
              }

              const lm = det.landmarks;
              srcPts = [
                averagePoint(lm.getLeftEye()),
                averagePoint(lm.getRightEye()),
                lm.getNose()[3]
              ];
              updateStatus('Source face ready! Face swap active.');
            } catch (err) {
              updateStatus('Error processing source image: ' + err.message, true);
              srcImg = null;
            }
          };
          srcImg.src = dataURL;
        } catch (err) {
          updateStatus('Error reading uploaded file: ' + err.message, true);
        }
      };
      input.click();
    });

    // 4) Main loop: detect live face, compute transform, draw
    async function swapLoop() {
      const size = { width: video.width, height: video.height };
      faceapi.matchDimensions(canvas, size);

      let lastDrawTime = 0;
      const minDrawInterval = 50; // Maximum 20 FPS to reduce CPU usage

      async function processFrame() {
        const now = Date.now();
        if (now - lastDrawTime < minDrawInterval) {
          requestAnimationFrame(processFrame);
          return;
        }
        lastDrawTime = now;

        if (!srcImg || !srcPts) {
          requestAnimationFrame(processFrame);
          return;
        }

        try {
          const det = await faceapi
            .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks();

          ctx.clearRect(0, 0, canvas.width, canvas.height);

          if (det) {
            const lm = det.landmarks;
            const dstPts = [
              averagePoint(lm.getLeftEye()),
              averagePoint(lm.getRightEye()),
              lm.getNose()[3]
            ];

            const M = getAffineTransform(srcPts, dstPts);
            ctx.save();
            ctx.globalAlpha = 0.9; // Slightly transparent for better blending
            ctx.setTransform(M.a, M.b, M.c, M.d, M.e, M.f);
            ctx.drawImage(srcImg, 0, 0);
            ctx.restore();
          }
        } catch (err) {
          console.error('Frame processing error:', err);
        }

        requestAnimationFrame(processFrame);
      }

      processFrame();
    }

    // Helpers
    function averagePoint(arr) {
      const sum = arr.reduce((a, p) => ({ x: a.x + p.x, y: a.y + p.y }), { x: 0, y: 0 });
      return { x: sum.x / arr.length, y: sum.y / arr.length };
    }

    function getAffineTransform(src, dst) {
      const [x0,y0,x1,y1,x2,y2,u0,v0,u1,v1,u2,v2] =
        [src[0].x,src[0].y,src[1].x,src[1].y,src[2].x,src[2].y,
         dst[0].x,dst[0].y,dst[1].x,dst[1].y,dst[2].x,dst[2].y];
      const denom = x0*(y1-y2)+x1*(y2-y0)+x2*(y0-y1);
      const a = (u0*(y1-y2)+u1*(y2-y0)+u2*(y0-y1))/denom;
      const b = (v0*(y1-y2)+v1*(y2-y0)+v2*(y0-y1))/denom;
      const c = (u0*(x2-x1)+u1*(x0-x2)+u2*(x1-x0))/denom;
      const d = (v0*(x2-x1)+v1*(x0-x2)+v2*(x1-x0))/denom;
      const e = (u0*(x1*y2-x2*y1)+u1*(x2*y0-x0*y2)+u2*(x0*y1-x1*y0))/denom;
      const f = (v0*(x1*y2-x2*y1)+v1*(x2*y0-x0*y2)+v2*(x0*y1-x1*y0))/denom;
      return {a,b,c,d,e,f};
    }
  </script>
</body>
</html>